{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599ce63e",
   "metadata": {},
   "source": [
    "Notebook for cleaning the peptide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66d40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alden/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Supress pytorch pickle load warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Library imports\n",
    "import gdiffusion as gd\n",
    "import util\n",
    "import util.chem as chem\n",
    "import util.visualization as vis\n",
    "import util.stats as gdstats\n",
    "\n",
    "\n",
    "import gdiffusion.bayesopt as bayesopt\n",
    "from gdiffusion.classifier.logp_predictor import LogPPredictor\n",
    "\n",
    "device = util.util.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "DIFFUSION_PATH = \"saved_models/diffusion/molecule-diffusion-v1.pt\"\n",
    "SELFIES_VAE_PATH = \"saved_models/selfies_vae/selfies-vae.ckpt\"\n",
    "PEPTIDE_VAE_PATH = \"saved_models/peptide_vae/peptide-vae.ckpt\"\n",
    "LOGP_PREDICTOR_PATH = \"saved_models/logp/model-logp\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93816bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,labels\n",
      "\n",
      "FLPQGTPSPLIPMLILIETISLFIQPMALAVRLTANITAGHLLIHL,1\n",
      "\n",
      "VMATAFMGYVLPWGQMSFWGATVITNLLSAIPYIGPTLVEWIW,0\n",
      "\n",
      "QDIRKMGGMMYTLPFTSSCLMIGTLALTGMPFMTGFYSKDHII,1\n",
      "\n",
      "AFMGYVLPWGQMSFWGATVITNLLSAIPYIGTTLVEW,0\n",
      "\n",
      "MLTMIPILMKTTNPRSTEAATKYFMTQATASMMLMMALTINLVYS,1\n",
      "\n",
      "LLVLFIMFQLKVSNHMYPMNPELIKPKLKEQKTPWE,1\n",
      "\n",
      "QCPKPTLQQISHIAQQLGLEKDVVRVWFCNRRQKGKRSSSDYSQREDF,0\n",
      "\n",
      "LASATNTWEIQQL,0\n",
      "\n",
      "IQQAFSHTQAPTLPLLGLILAATGKSAQ,0\n",
      "\n",
      "MAIAMLSLLSLFFYLRLAYHSTIILPPNSSNH,1\n",
      "\n",
      "DVIRESTFQGHHTTTVQKGLRYGMVLFIVSEVFFFLGFFW,1\n",
      "\n",
      "MISHIVTYYSGKKEPFGYMGMVWAMVSIGFLGFIVWA,0\n",
      "\n",
      "PILIAMAFLMLTERKILGYMQLRKGPNVVGPYGL,0\n",
      "\n",
      "IPMITNSLT,1\n",
      "\n",
      "PWASQTSKLPTMLITALL,0\n",
      "\n",
      "PPLSGFLPKWMIIQEMTKNSLIIMPTMMAI,1\n",
      "\n",
      "ALMVALAICSLVLYLLTLMLTEKLSS,0\n",
      "\n",
      "ADAIKLFTKEPLKPSTS,0\n",
      "\n",
      "LLILVLFLPDLLGDPDNYTPANPLN,1\n",
      "\n",
      "IIMYNPTLMALNLIIYLLMT,1\n",
      "\n",
      "IPGGPFENLEIRRFDRVKDTEWNDFEYRFIS,0\n",
      "\n",
      "ASEPYTTKFFYYLLMFLI,1\n",
      "\n",
      "NFTPANPLATPPHIKPEWYFLFAYAILRSIPNKLGG,0\n",
      "\n",
      "SMLPIILLVFAAC,0\n",
      "\n",
      "NVFGFKALRALRLEDLRIPTAYVKTFQGPPHGIQVERDKLNKYGRPLLGC,0\n",
      "\n",
      "AKMPLYGLHLWLPKA,0\n",
      "\n",
      "KDAIFSVSIAYFGIFIASF,0\n",
      "\n",
      "NMSFWLLPPSFLLLLASSTVEAGAGTGWTVYPPLAGNMAH,0\n",
      "\n",
      "KILGYMQLRKGPNIVGPLGLLQPMADGVKLFIK,1\n",
      "\n",
      "YLLFYTLISSLPLLVTILYLHMQIGTLHLTVLKLTPSALNNSWADLLS,1\n",
      "\n",
      "MTRNLMFKQPSYMHKFSNMLGYYPTTTHRILPYLGLTMSQNLATTIMD,1\n",
      "\n",
      "SNTQLYKNNLYPHYVKTTIFYAFTI,0\n",
      "\n",
      "AHVSTISICIYLNDATNLH,0\n",
      "\n",
      "IEEHGWAHWAFAPFRLAHALLAALAPARTDRGGDGDGG,0\n",
      "\n",
      "LFSAVPYIGQALVEWAWGGFSVDNP,0\n",
      "\n",
      "TGPAGARGPGPPGKAGE,1\n",
      "\n",
      "SKKAELSPPEKYMKVASPSVKEQFNQNTNV,0\n",
      "\n",
      "MFMINILSLIIP,0\n",
      "\n",
      "NSPTTITRTVKTAFMVSLIPMITFIYSGTEHIIFHWECKPVINFKIP,1\n",
      "\n",
      "VGFVGFSSKPSPIYGGLVLIVSGGVGCGIVLNFGGSFLGLMVFLIY,1\n",
      "\n",
      "LANGVKLTDNQLVVPLDGLYLIYSQVLFKGQGCP,1\n",
      "\n",
      "GGMLVVFVYSVSLAADPFPEAWGDWRVMGYGLGFVLVLVVGVVVGGFVGE,0\n",
      "\n",
      "SQFGFQDASSPIMEELVEFHDHALMVALAICSLVLYLLMLML,0\n",
      "\n",
      "LAVTILGFMLALELSLMTHNLKLEHSTSVFKFSNLLGYYPTIMHRLPPLA,1\n",
      "\n",
      "LILMLALLLTLTLFSPN,0\n",
      "\n",
      "FLLYEYDIFWAFLIISSLIPILAFFISGILAPIRKGPEKLSSYESG,0\n",
      "\n",
      "GGLVLIVSGGVGCGIVLNFGGSF,1\n",
      "\n",
      "VGEPGPLGISGPPGARGPPGAVGPGVN,1\n",
      "\n",
      "EDWMIFGGKEGSVIREDSLGVASLYNITSWFMVVAGWSLFISVL,1\n",
      "\n",
      "TRIIFYALLNQPRFPPLILINENNPLLINSIKRL,0\n",
      "\n",
      "KFLWTNTTTYSLLIATLS,0\n",
      "\n",
      "EVDNRMVLPMDLPVRVLVSSEDVLH,1\n",
      "\n",
      "NPIKRLAAGSLFAGFLITNNISPASPFQTTIPLYLKLTALAVTFLGL,0\n",
      "\n",
      "SPLIFTTI,0\n",
      "\n",
      "MVNPSPWPLTGALSALLMTSGLIMWFHFNSTYLL,0\n",
      "\n",
      "LLEIPQDLPPSLQLLSLEANNI,0\n",
      "\n",
      "PVGLPGIDGRPGPVGPAGR,1\n",
      "\n",
      "TLAANLFILTWIGSQPVEHPFIIIGQLASLTYFTII,0\n",
      "\n",
      "EYTNIIMMNTLTTTIFLGTTYNALSPELYT,0\n",
      "\n",
      "AFSACEAGAGLAMLVASTRTH,0\n",
      "\n",
      "STNNLLHYPFIALALWGALMTSSICLRQIDLKSLIAYSSVSHMGLV,1\n",
      "\n",
      "QLDTSTWLTMILSMFLTLFIIFQLKISKHNFHYNPELTSTKMLKQ,0\n",
      "\n",
      "TGLFLAMHYTSDTMTAFSSVTHICRDVNYGWIIRYLHANGASMFFIC,0\n",
      "\n",
      "SLRILYMMDEINNPSLTVKTMGHQWYWSYEYTDYGD,0\n",
      "\n",
      "LWSGWASNSKYALIGALRAVAQTISYEVSLAIIL,0\n",
      "\n",
      "THPQTVSEDICVQKLVTDSSPCKNKNAAT,1\n",
      "\n",
      "ESSTGTWTTVWTDR,0\n",
      "\n",
      "PYGLLQPIADALKLFIKEPLQPLTSSTSMFIIAPIL,1\n",
      "\n",
      "PRAIEAAIKYFLVQAAASTLVLFSSTINAWHSGQWDLTQLTHPTSCSL,1\n",
      "\n",
      "RTFHTGGVFTGGTAEHVRAPSNGKIKFNEDLVHPTRTRHGHPAFLCS,0\n",
      "\n",
      "FYTPKGSDHSSITTRI,0\n",
      "\n",
      "STWFTTILSMVLTLFIMFQLTLSKHHYPKAPESKAPMPLKTKTP,1\n",
      "\n",
      "DFKDLTFDSYMTPTTDLPLGHFRLLEVDHRVVVPMESPVRVIVTA,0\n",
      "\n",
      "LSLLIRAELGQPGTLLGDDQ,0\n",
      "\n",
      "FCLSNTNYERIH,0\n",
      "\n",
      "GIPGPPGAPGPQG,0\n",
      "\n",
      "LLLTVPIMMTSLNTYKSSNYPLYVKTT,0\n",
      "\n",
      "FITSLVPMSLFMYSGSESIISHLEWKFIMNFKIPISLKMDQYSLMFLPIA,1\n",
      "\n",
      "LALGLLSMLLVMLQWWRDVVRESTFQGHHTP,0\n",
      "\n",
      "NTTPLKTQPMTMPIHIKIAALTVTVLGLLLALELTTMTN,1\n",
      "\n",
      "TLTTLLLPIMLPLISPKLQNTPS,1\n",
      "\n",
      "TVSKDLNPMASIVMTAALAMKLGLAPFHFWVP,1\n",
      "\n",
      "LMTLVATSLTAVYSTRIIF,0\n",
      "\n",
      "FPVVYIGAIAVSFLFVVMMFNIQIAEIHEEVLRYLPVSGIIGLIFWW,0\n",
      "\n",
      "TLVLGITLGVLVIVWLEREISAGIQQRIGPEY,0\n",
      "\n",
      "TSTMDAQEVETIWTILPAIILILIALPSLQ,1\n",
      "\n",
      "INFHFTSNHHFGF,0\n",
      "\n",
      "IFLVARLLPLFIVIPYIMNLISFIGIITVLLGATLALAQKD,0\n",
      "\n",
      "LAMHYTSDTMTAFSSVTHICRDVNYGWLIRYMHANGASMFFICLF,0\n",
      "\n",
      "YMLPILIAVAFLTLIERKILGYMQLRKGPNIVGPL,1\n",
      "\n",
      "SDQTSSTIMTLALAMKLGLAPFHFWVPEVTQGIP,0\n",
      "\n",
      "VQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPRSHPFFTR,0\n",
      "\n",
      "ELTPTNVEWLHGCPPPYHTYEEPAHVQT,0\n",
      "\n",
      "FLVMFIGVNLTFFPQHFLGLSGMPRRYSDYPDAYTAWNTASSMGSFISLV,0\n",
      "\n",
      "MEGNRKNMLQALLITILLGVYFTL,0\n",
      "\n",
      "MIISMLSLLGLFFYLRLAYYATITLPPNSANHMKQWHTNKHTSSPTAVLT,1\n",
      "\n",
      "GSLHMPMMSIMN,1\n",
      "\n",
      "VFAILAGLTHWFPLF,0\n",
      "\n",
      "ALLLTLLATTFTATYSLRMAMLIQTGH,1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file=\"data/raw_peptide/peptide_raw_10M.csv\") as f:\n",
    "    for i in range(100):\n",
    "        print(f.readline())\n",
    "\n",
    "# data is PEPTIDE, EXTINCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e733ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence\n",
      "\n",
      "QRSTPYCRQSIPKGTIV\n",
      "\n",
      "STPYCRQSIPKGTIVPLKGP\n",
      "\n",
      "GIISHIWALARHTLFTNTFQDDER\n",
      "\n",
      "TGTGNALRRRATSVATSVGTD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/raw_peptide/peptide_raw_4p5.csv\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline())\n",
    "\n",
    "# all non-extinct (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3c7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset lengths:\n",
    "def get_num_lines(file_name):\n",
    "    total_lines = 0\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                total_lines += 1\n",
    "    return total_lines\n",
    "\n",
    "peptide_10m_len = get_num_lines(\"data/raw_peptide/peptide_raw_10M.csv\")\n",
    "peptide_4p5_len = get_num_lines(\"data/raw_peptide/peptide_raw_4p5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c489bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Million Peptide Len: 10274724\n",
      "4.5 Million Peptide Len: 4500001\n"
     ]
    }
   ],
   "source": [
    "print(f\"10 Million Peptide Len: {peptide_10m_len}\")\n",
    "print(f\"4.5 Million Peptide Len: {peptide_4p5_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract 2 for the header lines\n",
    "total_len = peptide_10m_len + peptide_4p5_len - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "294da046",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create h5py file, do not run again!\n",
    "\n",
    "# # data source is if its from the 10M dataset (0) or the 4.5M dataset (1)\n",
    "# dataset_file = \"data/peptide_dataset.h5\"\n",
    "# with h5py.File(dataset_file, 'w') as h5file:\n",
    "#     peptide_dataset = h5file.create_dataset('PEPTIDES', (total_len), dtype=h5py.string_dtype())\n",
    "#     extinct_dataset = h5file.create_dataset('EXTINCT', (total_len), dtype=bool)\n",
    "#     data_source = h5file.create_dataset('DATA_SOURCE', (total_len), dtype=np.int8)\n",
    "#     latents = h5file.create_dataset('LATENTS', (total_len, peptide_latent_dim), dtype=np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058a7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from peptide latent data:\n",
    "def read_peptide_dataset_raw(i: int, data_path=\"data/peptide_dataset.h5\"):\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        return f['PEPTIDES'][i], f['EXTINCT'][i], f['DATA_SOURCE'][i], f['LATENTS'][i]\n",
    "    \n",
    "def read_peptide_dataset(i: int, data_path=\"data/peptide_dataset.h5\"):\n",
    "    ''' return peptide, latent, extinct, datasource '''\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        raw_peptide, raw_extinct, raw_datasource, raw_latent = f['PEPTIDES'][i], f['EXTINCT'][i], f['DATA_SOURCE'][i], f['LATENTS'][i]\n",
    "        peptide = raw_peptide.decode('utf-8')\n",
    "        extinct = bool(raw_extinct)\n",
    "        datasource = 'peptide_10M' if raw_datasource == 0 else 'peptide_4.5M'\n",
    "        latent = raw_latent\n",
    "\n",
    "    return peptide, latent, extinct, datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71337a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_10m_len = 10274724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a50480f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"data/raw_peptide/peptide_raw_10M.csv\"\n",
    "PEPTIDE_DATASET_PATH = \"data/peptide_dataset.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88ef5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH_4P5 = \"data/raw_peptide/peptide_raw_4p5.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Peptide10M CSV: 100%|█████████▉| 10274723/10274724 [16:58<00:00, 10089.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# def write_peptide_10M_dataset(start_idx: int = 0, start_line_num: int = 1):\n",
    "#     # line_num is 0-indexed, so line_num 0 is the first line!\n",
    "    \n",
    "#     with open(RAW_DATA_PATH, 'r') as infile, h5py.File(PEPTIDE_DATASET_PATH, 'r+') as outfile:\n",
    "#         peptide_ds = outfile['PEPTIDES']\n",
    "#         extinct_ds = outfile['EXTINCT']\n",
    "#         data_source_ds = outfile['DATA_SOURCE']\n",
    "\n",
    "#         # skip first start_line_num lines in infile csv\n",
    "#         for _ in range(start_line_num):\n",
    "#             next(infile)\n",
    "\n",
    "#         idx = start_idx\n",
    "#         for line_num, raw_line in tqdm(enumerate(infile, start=start_line_num), total=peptide_10m_len, desc='Reading Peptide10M CSV'):\n",
    "#             try:\n",
    "#                 raw_line = raw_line.strip()\n",
    "#                 peptide, extinct = raw_line.split(',')\n",
    "\n",
    "#                 if peptide is None or extinct is None:\n",
    "#                     raise ValueError(f\"peptide, extinct is wrong: peptide={peptide} extinct={extinct}\")\n",
    "\n",
    "#                 if peptide_ds[idx] != b'':\n",
    "#                     print(f\"Warning, overriding peptide data: {peptide_ds[idx]}! Aborting\")\n",
    "#                     raise ValueError(\"See above.\")\n",
    "                \n",
    "#                 peptide_ds[idx] = peptide\n",
    "#                 extinct_ds[idx] = extinct\n",
    "#                 data_source_ds[idx] = 0 # this coorosponds to the 10M dataset\n",
    "#                 idx += 1\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Encountered an error while processing line_num {line_num}\")\n",
    "#                 print(f\"Line was {raw_line} \")\n",
    "#                 print(f\"peptide = {peptide} \")\n",
    "#                 print(f\"extinct = {extinct} \")\n",
    "#                 print(f\"Attempted to index into idx={idx}\")\n",
    "#                 print(\"Error MSG:\")\n",
    "#                 print(e)\n",
    "\n",
    "#                 print(f\"Removing idx: {idx} data\")\n",
    "#                 peptide_ds[idx] = ''\n",
    "#                 extinct_ds[idx] = False\n",
    "#                 data_source_ds[idx] = 0\n",
    "#                 return\n",
    "\n",
    "# # write_peptide_10M_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99269ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Peptide 4.5M CSV: 100%|█████████▉| 4500000/4500001 [07:29<00:00, 10001.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# # TODO: Determine start_idx\n",
    "# def write_peptide_4p5M_dataset(start_idx: int = None, start_line_num: int = 1):\n",
    "#     # line_num is 0-indexed, so line_num 0 is the first line!\n",
    "    \n",
    "#     with open(RAW_DATA_PATH_4P5, 'r') as infile, h5py.File(PEPTIDE_DATASET_PATH, 'r+') as outfile:\n",
    "#         peptide_ds = outfile['PEPTIDES']\n",
    "#         extinct_ds = outfile['EXTINCT']\n",
    "#         data_source_ds = outfile['DATA_SOURCE']\n",
    "\n",
    "#         # skip first start_line_num lines in infile csv\n",
    "#         for _ in range(start_line_num):\n",
    "#             next(infile)\n",
    "\n",
    "#         idx = start_idx\n",
    "#         for line_num, raw_line in tqdm(enumerate(infile, start=start_line_num), total=peptide_4p5_len, desc='Reading Peptide 4.5M CSV'):\n",
    "#             try:\n",
    "#                 peptide = raw_line.strip()\n",
    "\n",
    "#                 if peptide is None:\n",
    "#                     raise ValueError(f\"peptide, extinct is wrong: peptide={peptide}\")\n",
    "\n",
    "#                 if peptide_ds[idx] != b'':\n",
    "#                     print(f\"Warning, overriding peptide data: {peptide_ds[idx]}! Aborting\")\n",
    "#                     raise ValueError(\"See above.\")\n",
    "                \n",
    "#                 peptide_ds[idx] = peptide\n",
    "#                 extinct_ds[idx] = False # these are all modern peptides, so they are all NOT extinct\n",
    "#                 data_source_ds[idx] = 1 # this coorosponds to the 4.5 dataset\n",
    "#                 idx += 1\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Encountered an error while processing line_num {line_num}\")\n",
    "#                 print(f\"Line was {raw_line} \")\n",
    "#                 print(f\"peptide = {peptide} \")\n",
    "#                 print(f\"Attempted to index into idx={idx}\")\n",
    "#                 print(\"Error MSG:\")\n",
    "#                 print(e)\n",
    "\n",
    "#                 print(f\"Removing idx: {idx} data\")\n",
    "#                 peptide_ds[idx] = ''\n",
    "#                 extinct_ds[idx] = False\n",
    "#                 data_source_ds[idx] = 0\n",
    "#                 return\n",
    "\n",
    "# peptide_start_index = peptide_10m_len - 1\n",
    "# # write_peptide_4p5M_dataset(start_idx=peptide_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 684970/14774723 [00:32<11:14, 20879.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m peptide \u001b[38;5;241m=\u001b[39m peptide_ds[idx]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m extinct \u001b[38;5;241m=\u001b[39m extinct_ds[idx]\n\u001b[0;32m---> 11\u001b[0m data_source \u001b[38;5;241m=\u001b[39m \u001b[43mextinct_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(peptide) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSmall Peptide peptide at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeptide\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/h5py/_hl/dataset.py:804\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is this dataset suitable for simple reading\"\"\"\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extent_type \u001b[38;5;241m==\u001b[39m h5s\u001b[38;5;241m.\u001b[39mSIMPLE\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mget_type(), (h5t\u001b[38;5;241m.\u001b[39mTypeIntegerID, h5t\u001b[38;5;241m.\u001b[39mTypeFloatID))\n\u001b[1;32m    802\u001b[0m     )\n\u001b[0;32m--> 804\u001b[0m \u001b[38;5;129m@with_phil\u001b[39m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, new_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    806\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Read a slice from the HDF5 dataset.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03m    Takes slices and recarray-style field names (more than one is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m    * Boolean \"mask\" array indexing\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (args,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Confirming data:\n",
    "peptide_amino_acids = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "with h5py.File(\"data/peptide_dataset.h5\", mode='r') as f:\n",
    "    peptide_ds = f['PEPTIDES']\n",
    "    extinct_ds = f['EXTINCT']\n",
    "    data_source_ds = f['DATA_SOURCE']\n",
    "\n",
    "    for idx in tqdm(range(len(peptide_ds))):\n",
    "        peptide = peptide_ds[idx].decode('utf-8')\n",
    "        extinct = extinct_ds[idx]\n",
    "        data_source = extinct_ds[idx]\n",
    "\n",
    "        if len(peptide) <= 4:\n",
    "            print(f\"Small Peptide peptide at index {idx}: {peptide}\")\n",
    "\n",
    "        if not peptide.isupper():\n",
    "            print(f\"Peptide at index {idx} is not uppercase: {peptide}\")\n",
    "        \n",
    "        if not all(c in peptide_amino_acids for c in peptide):\n",
    "            print(f\"Peptide at index {idx} is not a valid peptide! {peptide}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e34bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def peptides_similar(seq1, seq2, max_changes=5):\n",
    "   matcher = SequenceMatcher(None, seq1, seq2)\n",
    "   opcodes = matcher.get_opcodes()\n",
    "   changes = sum(1 for tag, _, _, _, _ in opcodes if tag != 'equal')\n",
    "   return changes <= max_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61a5fcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Peptide: ALAPRHADVVAPRLMAITRAGVTALVLTAFLGVRGLNPGADLL\n",
      "    Extinct: False\n",
      "    Data Source: peptide_4.5M\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "peptide, _, extinct, data_source = read_peptide_dataset(total_len-1)\n",
    "string_print = f\"\"\"\n",
    "    Peptide: {peptide}\n",
    "    Extinct: {str(extinct)}\n",
    "    Data Source: {data_source}\n",
    "    \"\"\"\n",
    "print(string_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "097430e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from saved_models/peptide_vae/peptide-vae.ckpt\n",
      "Enc params: 2,675,904\n",
      "Dec params: 360,349\n"
     ]
    }
   ],
   "source": [
    "# attatch latents to dataset (this will take a long long time)\n",
    "\n",
    "vae = gd.load_vae_peptides()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 0 --- Start Idx: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:36<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 1 --- Start Idx: 64000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 2 --- Start Idx: 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:34<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 3 --- Start Idx: 192000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:34<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 4 --- Start Idx: 256000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:36<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 5 --- Start Idx: 320000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:35<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 6 --- Start Idx: 384000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:34<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 7 --- Start Idx: 448000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:42<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 8 --- Start Idx: 512000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block: 100%|██████████| 1000/1000 [04:39<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Number: 9 --- Start Idx: 576000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE Block:  11%|█         | 110/1000 [00:30<04:08,  3.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae_batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvae_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrupted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(vae_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m vb \u001b[38;5;241m=\u001b[39m \u001b[43mattatch_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[117], line 24\u001b[0m, in \u001b[0;36mattatch_latents\u001b[0;34m(start_idx, vae_batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m             vae_batch \u001b[38;5;241m=\u001b[39m peptide_block[i:i \u001b[38;5;241m+\u001b[39m vae_batch_size]\n\u001b[1;32m     23\u001b[0m             vae_batch \u001b[38;5;241m=\u001b[39m [vae_ele\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m vae_ele \u001b[38;5;129;01min\u001b[39;00m vae_batch]\n\u001b[0;32m---> 24\u001b[0m             latents \u001b[38;5;241m=\u001b[39m \u001b[43mgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeptides_to_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     25\u001b[0m             latents_ds[block_idx \u001b[38;5;241m+\u001b[39m i:block_idx \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vae_batch)] \u001b[38;5;241m=\u001b[39m latents\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Research/GDProject/gdiffusion/vae/util.py:290\u001b[0m, in \u001b[0;36mpeptides_to_latent\u001b[0;34m(peptides, vae)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m peptide_to_latent_helper(peptides, vae)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peptides, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([peptide_to_latent_helper(p, vae) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m peptides], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Research/GDProject/gdiffusion/vae/util.py:290\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m peptide_to_latent_helper(peptides, vae)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peptides, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mpeptide_to_latent_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m peptides], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Research/GDProject/gdiffusion/vae/util.py:279\u001b[0m, in \u001b[0;36mpeptide_to_latent_helper\u001b[0;34m(peptide, vae)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    278\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mpeptide_to_tokens(peptide)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 279\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     z \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu_ign\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_ign\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_ign\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vae\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mn_acc \u001b[38;5;241m*\u001b[39m vae\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39md_bnk)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/GDProject/gdiffusion/vae/peptide_molformers/models/BaseVAESwiGLURope.py:109\u001b[0m, in \u001b[0;36mBaseVAE.forward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m--> 109\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     z \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(sigma)\u001b[38;5;241m*\u001b[39msigma\n\u001b[1;32m    112\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z, tokens)\n",
      "File \u001b[0;32m~/Research/GDProject/gdiffusion/vae/peptide_molformers/models/BaseVAESwiGLURope.py:88\u001b[0m, in \u001b[0;36mBaseVAE.encode\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     85\u001b[0m pad_mask \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_tok\n\u001b[1;32m     86\u001b[0m pad_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mfull((tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_acc), \u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice), pad_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_acc]\n\u001b[1;32m     89\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_neck(encoding)\n\u001b[1;32m     91\u001b[0m mu, sigma \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/hnn_utils/nn/Transformer.py:38\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_pad_mask, is_causal)\u001b[0m\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 38\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/hnn_utils/nn/Transformer.py:136\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_pad_mask, is_causal)\u001b[0m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[0;32m--> 136\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/hnn_utils/nn/Transformer.py:151\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, pad_mask, is_causal)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    146\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/hnn_utils/nn/Transformer.py:336\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x, pad_mask, attn_mask, is_causal)\u001b[0m\n\u001b[1;32m    332\u001b[0m v \u001b[38;5;241m=\u001b[39m rearrange(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... n (h d) -> ... h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m    334\u001b[0m mask \u001b[38;5;241m=\u001b[39m combine_masks(attn_mask, pad_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 336\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_causal:\n\u001b[1;32m    339\u001b[0m     cm \u001b[38;5;241m=\u001b[39m causal_mask(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/hnn_utils/nn/RoPE.py:21\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk)\u001b[0m\n\u001b[1;32m     14\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m precompute_freqs_cis(\n\u001b[1;32m     15\u001b[0m     dim\u001b[38;5;241m=\u001b[39mD,\n\u001b[1;32m     16\u001b[0m     sl\u001b[38;5;241m=\u001b[39mSL,\n\u001b[1;32m     17\u001b[0m     device\u001b[38;5;241m=\u001b[39mxq\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     18\u001b[0m )\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, SL, D \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     20\u001b[0m xq_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(rearrange(xq\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h sl (d n) -> b h sl d n\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 21\u001b[0m xk_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb h sl (d n) -> b h sl d n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m xq_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xq_ \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m xk_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xk_ \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/einops/einops.py:600\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths: Size) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/einops/einops.py:532\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    530\u001b[0m     shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[1;32m    531\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhashable_axes_lengths\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error while processing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-reduction pattern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/einops/einops.py:243\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    241\u001b[0m     (init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added) \u001b[38;5;241m=\u001b[39m _result\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes_reordering \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mtranspose(tensor, axes_reordering)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-cuda/lib/python3.10/site-packages/einops/_backends.py:93\u001b[0m, in \u001b[0;36mAbstractBackend.reshape\u001b[0;34m(self, x, shape)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, shape):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def attatch_latents(start_idx: int = 576000, vae_batch_size=64):\n",
    "    num_batches_per_block = 1000\n",
    "    block_size = num_batches_per_block * vae_batch_size\n",
    "    \n",
    "    with h5py.File(PEPTIDE_DATASET_PATH, 'r+') as f:\n",
    "        peptide_ds = f['PEPTIDES']\n",
    "        latents_ds = f['LATENTS']\n",
    "        \n",
    "        block_num = start_idx // block_size\n",
    "        block_start_index = block_num * block_size\n",
    "        \n",
    "        try:\n",
    "            for block_idx in range(block_start_index, total_len, block_size):\n",
    "                print(f\"Block Number: {block_idx // block_size} --- Start Idx: {block_idx}\")\n",
    "                peptide_block = peptide_ds[block_idx:block_idx + block_size]\n",
    "                \n",
    "                # Fix: total should be number of batches, not number of items\n",
    "                num_batches = (len(peptide_block) + vae_batch_size - 1) // vae_batch_size\n",
    "                \n",
    "                for i in tqdm(range(0, len(peptide_block), vae_batch_size), \n",
    "                             total=num_batches, desc='VAE Block'):\n",
    "                    vae_batch = peptide_block[i:i + vae_batch_size]\n",
    "                    vae_batch = [vae_ele.decode('utf-8') for vae_ele in vae_batch]\n",
    "                    latents = gd.peptides_to_latent(vae_batch, vae=vae).cpu()\n",
    "                    latents_ds[block_idx + i:block_idx + i + len(vae_batch)] = latents\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Encountered error: {e}\")\n",
    "            print(f\"block_idx: {block_idx}\")\n",
    "            print(f\"block_num: {block_idx // block_size}\")\n",
    "            print(f\"i: {i}\")\n",
    "            print(f\"len(peptide_block): {len(peptide_block)}\")\n",
    "            print(f\"vae_batch_size: {vae_batch_size}\")\n",
    "            print(f\"corrupted: {block_idx + i} to {block_idx + i + len(vae_batch)}\")\n",
    "\n",
    "# vb = attatch_latents(start_idx=576000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b79b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.68168569e+00  4.36633170e-01  9.88466144e-01 -1.96448401e-01\n",
      " -2.22259223e-01 -9.87164736e-01  1.19704413e+00  6.30952358e-01\n",
      " -1.67972040e+00  9.32729840e-01 -7.07652092e-01 -1.29501843e+00\n",
      " -1.82481498e-01 -7.71961927e-01  1.76283073e+00  6.80858016e-01\n",
      " -1.60144761e-01 -1.15161479e+00 -2.11487770e+00 -1.61603570e+00\n",
      "  6.48729503e-04  2.86051452e-01  3.95024627e-01  1.51311064e+00\n",
      " -1.32089305e+00  3.64589095e-02 -2.96812952e-01  5.98318279e-01\n",
      " -8.72410178e-01 -9.16960716e-01 -5.62300444e-01  1.57823324e+00\n",
      " -5.04084945e-01  9.50967252e-01 -1.31093264e+00  1.34911346e+00\n",
      "  2.95524269e-01 -2.94118762e-01 -4.89745229e-01 -1.24121118e+00\n",
      "  1.99785769e-01  1.03533983e+00  7.79707670e-01  2.41386205e-01\n",
      " -5.03413618e-01 -3.80905360e-01 -2.55689216e+00 -8.02521288e-01\n",
      "  1.21050131e+00  4.75242376e-01 -1.31748855e+00  1.59162807e+00\n",
      "  3.91494453e-01  5.66990316e-01  1.40956819e+00  7.56166041e-01\n",
      " -2.00776860e-01 -1.17270410e-01  9.90917444e-01  8.68309379e-01\n",
      " -4.29015279e-01 -5.51364779e-01 -1.58050048e+00  3.37868303e-01\n",
      " -7.70379424e-01  6.76168501e-02 -1.26611531e+00 -1.20860624e+00\n",
      "  7.04645932e-01 -9.58486438e-01  2.24144840e+00  1.02820587e+00\n",
      "  2.86736131e-01 -2.22709194e-01  1.37896597e-01  2.84857452e-01\n",
      "  2.53274083e-01 -1.05804110e+00 -2.03558803e-02  8.48648846e-02\n",
      " -1.21296644e-02 -8.63425210e-02 -7.00076401e-01 -1.00552392e+00\n",
      "  1.17160156e-02  1.26165593e+00  1.64137352e+00  6.50884390e-01\n",
      " -1.08717728e+00 -3.68776768e-01  8.93020868e-01  7.92048991e-01\n",
      "  1.38465858e+00  1.80107892e-01 -3.51111919e-01 -7.61529058e-02\n",
      " -3.35215211e-01  9.29187775e-01  5.36569834e-01  2.15766668e+00\n",
      " -5.95781803e-01  2.73388684e-01  9.08417106e-01  4.10813421e-01\n",
      " -2.02924418e+00  6.06173635e-01  4.58819985e-01  8.06945562e-01\n",
      "  1.28249252e+00 -3.46579462e-01  8.87406051e-01  1.72346604e+00\n",
      "  2.84144789e-01 -1.24853754e+00  1.19725966e+00 -4.90314454e-01\n",
      " -4.55327332e-01  1.26121521e+00 -1.76707059e-01 -5.94258308e-03\n",
      "  5.66273928e-04  6.64637387e-01  5.55347562e-01  3.90622735e-01\n",
      "  8.63986254e-01 -5.41583896e-01 -3.06617975e-01 -2.98376083e-02\n",
      " -1.66917533e-01 -1.60694015e+00 -5.20423651e-01 -1.47915065e-01\n",
      "  2.18974084e-01 -3.75011086e-01 -1.12501055e-01  6.54265136e-02\n",
      " -4.46561873e-01 -1.73352063e+00  7.83871770e-01  1.01596737e+00\n",
      "  5.71544230e-01 -2.57339478e-01 -3.11738923e-02  1.43960819e-01\n",
      "  7.95467615e-01  9.46408987e-01 -3.32081825e-01 -2.49421433e-01\n",
      "  2.08364516e-01 -1.14535980e-01  9.63780284e-01 -7.88689330e-02\n",
      "  8.08027923e-01  1.21835518e+00  6.55960739e-01 -6.40419722e-01\n",
      "  1.86531067e+00  1.27360773e+00  1.73713279e+00 -1.40726638e+00\n",
      "  1.23790836e+00  4.28112984e-01 -1.20595682e+00 -1.08246863e+00\n",
      "  9.20072317e-01 -7.49506056e-03  6.93360418e-02  1.00614524e+00\n",
      "  1.20858538e+00  1.49137199e+00 -6.37891591e-02  1.62695980e+00\n",
      " -3.23252738e-01 -1.62785864e+00 -2.58363217e-01 -6.12857878e-01\n",
      "  6.22494221e-02  1.21343434e+00 -5.75460196e-01  8.33116651e-01\n",
      "  9.16272104e-01 -1.34471059e+00  1.02967632e+00  4.15931702e-01\n",
      "  2.67703682e-01  1.07371414e+00  3.41571599e-01 -2.14254782e-02\n",
      "  7.92177498e-01 -7.12667465e-01 -2.15739802e-01 -1.78445244e+00\n",
      " -1.61057138e+00 -3.43377590e-01  1.06731558e+00  9.78702068e-01\n",
      " -3.01199973e-01  5.12865067e-01  1.06988931e+00 -1.56434846e+00\n",
      " -1.51718402e+00 -1.56238997e+00 -3.65996361e-01  1.44917655e+00\n",
      "  4.80979264e-01  6.52392983e-01  1.84838325e-01  8.17634940e-01\n",
      " -6.43743575e-02  1.38190937e+00 -8.99735928e-01  6.65057302e-01\n",
      " -2.86256462e-01  1.23681426e-02  1.24779701e+00  5.49087226e-01\n",
      " -8.71906877e-01  5.45599282e-01  6.60927892e-02 -1.90300956e-01\n",
      "  1.65570843e+00  1.28858721e+00  2.47181833e-01  4.20103967e-01\n",
      "  1.98348022e+00  5.30262887e-01 -1.49409962e+00  3.50690722e-01\n",
      " -6.41120374e-02  6.28228188e-01 -4.36153352e-01  3.68937850e-04\n",
      "  2.70848215e-01  1.13227165e+00 -5.08068204e-02 -2.97352672e-01\n",
      " -1.41276193e+00 -1.63451803e+00  1.56056672e-01 -1.91974211e+00\n",
      "  1.28737259e+00  1.65099573e+00 -5.91073483e-02 -1.38049066e+00\n",
      "  7.95487285e-01  2.78600246e-01  1.16302633e+00  1.90576077e+00\n",
      " -6.48693800e-01 -1.95943087e-01  3.78000885e-01  1.78036511e-01\n",
      "  5.68743289e-01  1.62634468e+00 -5.44878840e-01 -1.33457339e+00]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import h5py\n",
    "\n",
    "# class PeptideDataset(Dataset):\n",
    "#     \"\"\"Dataset for the Peptides\"\"\"\n",
    "\n",
    "#     def __init__(self, file_loc:str =\"data/peptide_dataset.h5\", transform=None):\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             file_loc (string): Path to the peptide dataset\n",
    "#             transform: transform to be applied on a sample\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # keep file open\n",
    "#         self.file = h5py.File(file_loc, 'r')\n",
    "#         self.latent_dataset = self.file['LATENTS']\n",
    "#         self.peptide_dataset = self.file['PEPTIDES']\n",
    "#         self.extinct_dataset = self.file['EXTINCT']\n",
    "#         self.datasource_dataset = self.file['DATA_SOURCE']\n",
    "#         self._cached_len = len(self.peptide_dataset[:])\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self, use_cached=True):\n",
    "#         if use_cached:\n",
    "#             return self._cached_len\n",
    "#         else:\n",
    "#             return len(self.peptide_dataset[:])\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         peptide = self.get_peptide(idx)\n",
    "#         latent = self.get_latent(idx)\n",
    "#         extinct = self.get_extinct(idx)\n",
    "#         datasource = self.get_datasource(idx)\n",
    "\n",
    "#         out = (peptide, latent, extinct, datasource)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             out = self.transform(out)\n",
    "#         return out\n",
    "      \n",
    "\n",
    "#         out = ()\n",
    "#         raw_peptide, raw_extinct, raw_datasource, raw_latent = f['PEPTIDES'][i], f['EXTINCT'][i], f['DATA_SOURCE'][i], f['LATENTS'][i]\n",
    "\n",
    "#         peptide = raw_peptide.decode('utf-8')\n",
    "#         extinct = bool(raw_extinct)\n",
    "#         datasource = 'peptide_10M' if raw_datasource == 0 else 'peptide_4.5M'\n",
    "#         latent = raw_latent\n",
    "#         out = (self.smiles_dataset[idx], self.selfies_dataset[idx], self.latent_dataset[idx])\n",
    "#         if self.transform:\n",
    "#             out = self.transform(out)\n",
    "#         return out\n",
    "    \n",
    "#     def get_peptide(self, idx, raw=True):\n",
    "#         peptide = self.peptide_dataset[idx]\n",
    "\n",
    "#         if not raw:\n",
    "#             peptide = self.transform_peptide(peptide)\n",
    "#         return peptide\n",
    "    \n",
    "#     def transform_peptide(peptide):\n",
    "#         return [ptd.decode('utf-8') for ptd in peptide] if isinstance(peptide, list) else peptide.decode('utf-8')\n",
    "    \n",
    "#     def transform_extinct(extinct):\n",
    "\n",
    "\n",
    "#     def transform_datasource(datasource):\n",
    "        \n",
    "\n",
    "#         extinct = self.extinct_dataset[idx]\n",
    "#         if isinstance(extinct, list):\n",
    "#             extinct = [bool(ext) for ext in extinct]\n",
    "#         else:\n",
    "#             extinct = bool(extinct)\n",
    "\n",
    "#         datasource = self.datasource_dataset[idx]\n",
    "#         if isinstance(datasource, list):\n",
    "#             datasource = ['peptide_10M' if ds == 0 else 'peptide_4.5M' for ds in datasource]\n",
    "#         else:\n",
    "#             datasource = 'peptide_10M' if datasource == 0 else 'peptide_4.5M'\n",
    "\n",
    "#         latent = self.latent_dataset[idx]\n",
    "#     def get_peptide(self, idx, raw=True):\n",
    "#         return self.smiles_dataset[idx]\n",
    "#     def get_peptide(self, idx, raw=True):\n",
    "#         return self.smiles_dataset[idx]\n",
    "#     def get_peptide(self, idx, raw=True):\n",
    "#         return self.smiles_dataset[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32f961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(PEPTIDE_DATASET_PATH, 'r') as file:\n",
    "    latent_dataset = file['LATENTS']\n",
    "    peptide_dataset = file['PEPTIDES']\n",
    "    extinct_dataset = file['EXTINCT']\n",
    "    datasource_dataset = file['DATA_SOURCE']\n",
    "\n",
    "    print(extinct_dataset[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc29de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "peptide_10M\n"
     ]
    }
   ],
   "source": [
    "peptide, latent, extinct, source = read_peptide_dataset(i=peptide_10m_len-2)\n",
    "print(extinct)\n",
    "print(source)\n",
    "\n",
    "# inclusive!\n",
    "peptide_10m_start_idx = 0\n",
    "peptide_10m_end_idx = peptide_10m_len - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e3cb368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Peptide10M CSV: 100%|█████████▉| 10274723/10274724 [05:51<00:00, 29190.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fix error where I accidently set all of the extinct to true in the dataset:\n",
    "\n",
    "def fix_extinct_dataset(start_idx: int = 0, end_idx: int = peptide_10m_len - 2, start_line_num: int = 1):\n",
    "    # line_num is 0-indexed, so line_num 0 is the first line!\n",
    "    \n",
    "    with open(\"artifacts/raw_peptide/peptide_raw_10M.csv\", 'r') as infile, h5py.File(PEPTIDE_DATASET_PATH, 'r+') as outfile:\n",
    "        peptide_ds = outfile['PEPTIDES']\n",
    "        extinct_ds = outfile['EXTINCT']\n",
    "        data_source_ds = outfile['DATA_SOURCE']\n",
    "\n",
    "        # skip first start_line_num lines in infile csv\n",
    "        for _ in range(start_line_num):\n",
    "            next(infile)\n",
    "\n",
    "        idx = start_idx\n",
    "        for line_num, raw_line in tqdm(enumerate(infile, start=start_line_num), total=peptide_10m_len, desc='Reading Peptide10M CSV'):\n",
    "            try:\n",
    "                raw_line = raw_line.strip()\n",
    "                peptide, extinct = raw_line.split(',')\n",
    "                \n",
    "                extinct = True if extinct == '1' else False\n",
    "\n",
    "\n",
    "                if data_source_ds[idx] != 0:\n",
    "                    raise ValueError(f\"overriding wrong data source with peptide={peptide} extinct={extinct} at idx = {idx}\")\n",
    "\n",
    "                if peptide is None or extinct is None:\n",
    "                    raise ValueError(f\"peptide, extinct is wrong: peptide={peptide} extinct={extinct} at idx= {idx}\")\n",
    "                \n",
    "                extinct_ds[idx] = extinct\n",
    "                idx += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Encountered an error while processing line_num {line_num}\")\n",
    "                print(f\"Line was {raw_line} \")\n",
    "                print(f\"peptide = {peptide} \")\n",
    "                print(f\"extinct = {extinct} \")\n",
    "                print(f\"Attempted to index into idx={idx}\")\n",
    "                print(\"Error MSG:\")\n",
    "                print(e)\n",
    "\n",
    "                # print(f\"Removing idx: {idx} data\")\n",
    "                # peptide_ds[idx] = ''\n",
    "                # extinct_ds[idx] = False\n",
    "                # data_source_ds[idx] = 0\n",
    "                return\n",
    "\n",
    "# fix_extinct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply it to the new dataset\n",
    "\n",
    "# def transfer_dataset():\n",
    "#     # line_num is 0-indexed, so line_num 0 is the first line!\n",
    "\n",
    "#     with h5py.File(\"data/peptide_dataset.h5\", 'r') as infile, h5py.File(\"data/peptide_data_latents.h5\", 'r+') as outfile:\n",
    "#         extinct_ds_correct = infile['EXTINCT'][:]\n",
    "#         outfile['EXTINCT'][:] = extinct_ds_correct\n",
    "        \n",
    "# transfer_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a92d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from saved_models/peptide_vae/peptide-vae.ckpt\n",
      "Enc params: 2,675,904\n",
      "Dec params: 360,349\n"
     ]
    }
   ],
   "source": [
    "vae = gd.load_vae_peptides()\n",
    "import util.visualization as vis\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d81ca718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peptide: AYHMVNPSPWPLTGALSALLMTSGLIMWFHYNSMSLLMLGFTTNL\n",
      "Latent: Shape=torch.Size([256]), first 5 elements: tensor([ 1.2213,  0.8944,  1.1083, -0.5544, -0.3610])\n",
      "Latent Predicted: Shape=torch.Size([1, 256]), first 5 elements: tensor([ 1.0926,  1.2618,  1.4820, -0.9541, -0.7224], device='cuda:0')\n",
      "Predicted Peptide from latent: AYHMVNPSPWPLTGALSALLMTSGLIMWFHYNSMSLLMLGFTTNL\n"
     ]
    }
   ],
   "source": [
    "idx = 14543\n",
    "peptide, latent, extinct, _ = read_peptide_dataset(idx)\n",
    "latent = torch.tensor(latent, dtype=torch.float32)\n",
    "print(f\"Peptide: {peptide}\")\n",
    "print(f\"Latent: Shape={latent.shape}, first 5 elements: {latent[:5]}\")\n",
    "\n",
    "latent_hat = gd.peptides_to_latent([peptide], vae=vae)\n",
    "print(f\"Latent Predicted: Shape={latent_hat.shape}, first 5 elements: {latent_hat[0][:5]}\")\n",
    "\n",
    "peptide_hat = gd.latent_to_peptides(latent, vae=vae)\n",
    "print(f\"Predicted Peptide from latent: {peptide_hat[0]}\")\n",
    "# print(latent_hat)\n",
    "# # latent = torch.tensor(latent, dtype=torch.float32)\n",
    "# # print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
