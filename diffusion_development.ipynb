{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdbdd74",
   "metadata": {},
   "source": [
    "Notebook for the diffusion development program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d027639",
   "metadata": {},
   "source": [
    "The goal is a modular and correct version of the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a06d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0444525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "# small helper modules\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "    \n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def convert_image_to_fn(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "# normalization functions\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90393632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "\n",
    "# sin pos emb\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# changed nn.Conv2d to nn.Conv1d\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv1d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "# changed nn.Conv2d to nn.Conv1d\n",
    "# got rid of Rearrange string\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Conv1d(dim , default(dim_out, dim), 4, 2, 1)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = RMSNorm(dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "        # no dropout\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out)\n",
    "        self.block2 = Block(dim_out, dim_out)\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c n -> b (h c) n', h = self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b (h d) n')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# model\n",
    "\n",
    "class Unet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels = 3,\n",
    "        self_condition = False,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv1d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time embeddings\n",
    "\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n",
    "\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim)\n",
    "            fourier_dim = dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                ResnetBlock(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                ResnetBlock(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv1d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv1d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = ResnetBlock(init_dim * 2, init_dim, time_emb_dim = time_dim)\n",
    "        self.final_conv = nn.Conv1d(init_dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond = None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6211a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116813/2906567518.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(DIFFUSION_MODEL_LOCATION)\n"
     ]
    }
   ],
   "source": [
    "DIFFUSION_MODEL_LOCATION = \"saved_models/diffusion/molecule-diffusion-v1.pt\"\n",
    "def load_unet():\n",
    "    unet_model = Unet1D(\n",
    "        dim=128, \n",
    "        dim_mults=(1, 2, 4, 8), \n",
    "        channels=1\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(DIFFUSION_MODEL_LOCATION)\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for key in checkpoint['model'].keys():\n",
    "        if key.startswith(\"model.\"):\n",
    "            new_key = key[6:]  # remove the 'model.' prefix\n",
    "            new_state_dict[new_key] = checkpoint['model'][key]\n",
    "\n",
    "    unet_model.load_state_dict(new_state_dict)\n",
    "    return unet_model\n",
    "\n",
    "unet = load_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe37cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "from einops import reduce\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e55f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.diffusion.util import *\n",
    "import torch\n",
    "\n",
    "# abstract base class\n",
    "class BetaSchedule:\n",
    "    def get_betas(num_timesteps, *args):\n",
    "        pass\n",
    "    def get_schedule_name():\n",
    "        pass\n",
    "\n",
    "class BetaScheduleLinear(BetaSchedule):\n",
    "    def get_betas(num_timesteps):\n",
    "        \"\"\"\n",
    "        linear schedule, proposed in original ddpm paper\n",
    "        \"\"\"\n",
    "        scale = 1000 / num_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, num_timesteps, dtype = torch.float64)\n",
    "    def get_schedule_name():\n",
    "        return 'linear'\n",
    "\n",
    "class BetaScheduleSigmoid(BetaSchedule):\n",
    "    def get_betas(num_timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):\n",
    "        \"\"\"\n",
    "        sigmoid schedule\n",
    "        proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "        better for images > 64x64, when used during training\n",
    "        \"\"\"\n",
    "        steps = num_timesteps + 1\n",
    "        t = torch.linspace(0, num_timesteps, steps, dtype = torch.float64) / num_timesteps\n",
    "        v_start = torch.tensor(start / tau).sigmoid()\n",
    "        v_end = torch.tensor(end / tau).sigmoid()\n",
    "        alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0, 0.999)\n",
    "    def get_schedule_name():\n",
    "        return 'sigmoid'\n",
    "    \n",
    "class BetaScheduleCosine(BetaSchedule):\n",
    "    def get_betas(num_timesteps, s = 0.008):\n",
    "        \"\"\"\n",
    "        cosine schedule\n",
    "        as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "        \"\"\"\n",
    "        steps = num_timesteps + 1\n",
    "        t = torch.linspace(0, num_timesteps, steps, dtype = torch.float64) / num_timesteps\n",
    "        alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0, 0.999)\n",
    "    def get_schedule_name():\n",
    "        return 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4857ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from random import random\n",
    "# from functools import partial\n",
    "# from collections import namedtuple\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.amp import autocast\n",
    "\n",
    "# from einops import reduce\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# # diffusion model is a sort of wrapper class with different modules\n",
    "# # a diffusion class has a:\n",
    "# # - Betas\n",
    "# # - Sampler\n",
    "# # - Trainer\n",
    "\n",
    "# # base sampler class\n",
    "# class DiffusionSampler:\n",
    "#     pass\n",
    "\n",
    "# #  base trainer class\n",
    "# class DiffusionTrainer:\n",
    "#     def __init__(self, num_timesteps=1000):\n",
    "#         self.num_timesteps=1000\n",
    "\n",
    "# class DiffusionSamplerDDPM(DiffusionSampler):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "# class DiffusionTrainerPredV(DiffusionTrainer):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "# class DiffusionModel(nn.Module):\n",
    "#     def __init__(self, \n",
    "#         model: Unet1D, \n",
    "#         trainer: DiffusionTrainer, \n",
    "#         sampler: DiffusionSampler, \n",
    "#         beta_schedule: BetaSchedule,\n",
    "#         latent_dim: int = 128,\n",
    "#     ):\n",
    "\n",
    "#         # being a subclass of nn.Module allows us to register buffers\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.model = model \n",
    "#         self.trainer = trainer\n",
    "#         self.sampler = sampler\n",
    "#         self.beta_schedule = beta_schedule\n",
    "#         self.latent_dim = latent_dim\n",
    "\n",
    "#         betas = beta_schedule.get_betas()\n",
    "#         alphas = 1. - betas\n",
    "#         alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "#         alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec626a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion1D(nn.Module):\n",
    "    ''' Latent Diffusion with pred_v and DDPM sampling (for now)'''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Unet1D,\n",
    "        beta_schedule: BetaSchedule,\n",
    "        latent_dim: int = 128,\n",
    "        num_timesteps: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = beta_schedule.get_betas()\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # loss weight\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "        loss_weight = snr / (snr + 1)\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, clip_x_start=False, clip_min=-1.0, clip_max=1.0):\n",
    "        v = self.model(x, t)\n",
    "        maybe_clip = partial(torch.clamp, min = clip_min, max = clip_max) if clip_x_start else identity\n",
    "\n",
    "        x_start = self.predict_start_from_v(x, t, v)\n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised=False, clip_min=-1.0, clip_max=1.0):\n",
    "        preds = self.model_predictions(x, t, clip_denoised, clip_min=clip_min, clip_max=clip_max)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "     \n",
    "    def condition_mean(self, cond_fn, mean, variance, t):\n",
    "        \"\"\"\n",
    "        Compute the mean for the previous step, given a function cond_fn that\n",
    "        computes the gradient of a conditional log probability with respect to\n",
    "        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n",
    "        condition on y.\n",
    "        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n",
    "        \"\"\"\n",
    "        # this fixes a bug in the official OpenAI implementation:\n",
    "        # https://github.com/openai/guided-diffusion/issues/51 (see point 1)\n",
    "        # use the predicted mean for the previous timestep to compute gradient\n",
    "        gradient = cond_fn(mean, t)\n",
    "        new_mean = (\n",
    "            mean.float() + variance * gradient.float()\n",
    "        )\n",
    "        # print(\"gradient: \",(gradient.float()))\n",
    "        # print(\"gradient-mean: \",(variance * gradient.float()).mean())\n",
    "        return new_mean\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, cond_fn=None, clip_denoised=False):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "        model_mean, variance, model_log_variance, x_start = self.p_mean_variance(\n",
    "            x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = clip_denoised\n",
    "        )\n",
    "        if exists(cond_fn) and exists(guidance_kwargs):\n",
    "            model_mean = self.condition_mean(cond_fn, model_mean, variance, x, batched_times, guidance_kwargs)\n",
    "        \n",
    "        if exists(cond_fn) and not exists(guidance_kwargs):\n",
    "            print(\"Warning, cond_fn found, but guidance_kwargs not found. Diffusion will not be guided\")\n",
    "            \n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None, clip_denoised=True):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond, cond_fn, guidance_kwargs, clip_denoised=clip_denoised)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True)\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None, clip_denoised=True):\n",
    "        seq_length, channels = self.seq_length, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, seq_length), return_all_timesteps = return_all_timesteps, cond_fn=cond_fn, guidance_kwargs=guidance_kwargs, clip_denoised=clip_denoised)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast('cuda', enabled = False)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None):\n",
    "        b, c, n = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, n, device, seq_length, = *img.shape, img.device, self.seq_length\n",
    "        assert n == seq_length, f'seq_length must be {seq_length}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "from einops import reduce\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "class GaussianDiffusion1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        seq_length,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'sigmoid',\n",
    "        schedule_fn_kwargs = dict(),\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = False,\n",
    "        min_snr_loss_weight = False,\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert not (type(self) == GaussianDiffusion1D and model.channels != model.out_dim)\n",
    "        assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta_schedule_fn = cosine_beta_schedule\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            beta_schedule_fn = sigmoid_beta_schedule\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # loss weight\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = maybe_clipped_snr / snr\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = maybe_clipped_snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "        # unnorm = lambda x : normalize_to_neg_one_to_one(unnormalize_to_zero_to_one(x))\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "        # self.unnormalize = unnorm if auto_normalize else identity\n",
    "    \n",
    "        \n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False):\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "     \n",
    "    def condition_mean(self, cond_fn, mean,variance, x, t, guidance_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute the mean for the previous step, given a function cond_fn that\n",
    "        computes the gradient of a conditional log probability with respect to\n",
    "        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n",
    "        condition on y.\n",
    "        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n",
    "        \"\"\"\n",
    "        # this fixes a bug in the official OpenAI implementation:\n",
    "        # https://github.com/openai/guided-diffusion/issues/51 (see point 1)\n",
    "        # use the predicted mean for the previous timestep to compute gradient\n",
    "        gradient = cond_fn(mean, t, **guidance_kwargs)\n",
    "        new_mean = (\n",
    "            mean.float() + variance * gradient.float()\n",
    "        )\n",
    "        # print(\"gradient: \",(gradient.float()))\n",
    "        # print(\"gradient-mean: \",(variance * gradient.float()).mean())\n",
    "        return new_mean\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None, cond_fn=None, guidance_kwargs=None, clip_denoised=True):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "        model_mean, variance, model_log_variance, x_start = self.p_mean_variance(\n",
    "            x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = clip_denoised\n",
    "        )\n",
    "        if exists(cond_fn) and exists(guidance_kwargs):\n",
    "            model_mean = self.condition_mean(cond_fn, model_mean, variance, x, batched_times, guidance_kwargs)\n",
    "        \n",
    "        if exists(cond_fn) and not exists(guidance_kwargs):\n",
    "            print(\"Warning, cond_fn found, but guidance_kwargs not found. Diffusion will not be guided\")\n",
    "            \n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None, clip_denoised=True):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond, cond_fn, guidance_kwargs, clip_denoised=clip_denoised)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True)\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False, cond_fn=None, guidance_kwargs=None, clip_denoised=True):\n",
    "        seq_length, channels = self.seq_length, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, seq_length), return_all_timesteps = return_all_timesteps, cond_fn=cond_fn, guidance_kwargs=guidance_kwargs, clip_denoised=clip_denoised)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast('cuda', enabled = False)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None):\n",
    "        b, c, n = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, n, device, seq_length, = *img.shape, img.device, self.seq_length\n",
    "        assert n == seq_length, f'seq_length must be {seq_length}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d716a",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca192c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_diffusion_model(\n",
    "        unet_dim=128,\n",
    "        unet_dim_mults=(1, 2, 4, 8),\n",
    "\n",
    "        diffusion_latent_dim=128,\n",
    "        diffusion_timesteps=1000,\n",
    "        sampling_timesteps=1000,\n",
    "    ):\n",
    "\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "    '''\n",
    "        Utility for creating a diffusion model with a built in UNet\n",
    "    '''\n",
    "\n",
    "    # create the unet for the diffusion model\n",
    "    unet_model = Unet1D(\n",
    "        dim=unet_dim, \n",
    "        dim_mults=unet_dim_mults, \n",
    "        channels=1\n",
    "    ).to(device)\n",
    "\n",
    "    # TODO: Can add sampling timesteps\n",
    "    diffusion_model = GaussianDiffusion1D(\n",
    "        model=unet_model,\n",
    "        seq_length=diffusion_latent_dim,\n",
    "        timesteps=diffusion_timesteps,\n",
    "        objective='pred_v',\n",
    "        sampling_timesteps=sampling_timesteps\n",
    "    ).to(device)\n",
    "\n",
    "    # print basic info and what not to let user know model is created\n",
    "\n",
    "    total_params = sum(p.numel() for p in diffusion_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in diffusion_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Model created successfully\")\n",
    "    print(f\"- Total parameters: {total_params:,}\")\n",
    "    print(f\"- Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"- Model size: {total_params * 4 / (1024**2):.1f} MB\")\n",
    "    \n",
    "    try:\n",
    "        device = next(diffusion_model.parameters()).device\n",
    "        print(f\"- Device: {device}\")\n",
    "    except StopIteration:\n",
    "        print(\"No parameters found\")\n",
    "    \n",
    "    print(f\"- Model Name: {type(diffusion_model).__name__}\")\n",
    "\n",
    "    return diffusion_model\n",
    "\n",
    "def load_diffusion_model(model, model_path, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    print(f\"Loaded model successfully from {model_path}\")\n",
    "\n",
    "def sample_diffusion(model: GaussianDiffusion1D, batch_size=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = model.sample(batch_size=batch_size)\n",
    "        latents = latents.reshape(batch_size, -1)\n",
    "        return latents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created successfully\n",
      "- Total parameters: 57,314,049\n",
      "- Trainable parameters: 57,314,049\n",
      "- Model size: 218.6 MB\n",
      "- Device: cuda:0\n",
      "- Model Name: GaussianDiffusion1D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116813/3819632101.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model successfully from saved_models/diffusion/molecule-diffusion-v1.pt\n"
     ]
    }
   ],
   "source": [
    "model = create_diffusion_model()\n",
    "load_diffusion_model(model=model, model_path=\"saved_models/diffusion/molecule-diffusion-v1.pt\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2707c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:10<00:00, 98.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1345, -0.7898, -0.6277,  ...,  0.1792, -0.5265, -0.8932]],\n",
       "\n",
       "        [[ 0.9377, -0.3934,  0.8871,  ..., -1.0000,  1.0000,  0.3057]],\n",
       "\n",
       "        [[-0.2569,  0.4750,  0.1595,  ..., -0.5694,  0.8951, -0.9992]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6887, -0.8073, -0.0257,  ..., -0.9461, -0.4724, -0.7839]],\n",
       "\n",
       "        [[ 0.9595, -0.6181,  0.9384,  ..., -0.8665,  0.9544,  0.6606]],\n",
       "\n",
       "        [[ 0.3510, -1.0000, -0.8877,  ..., -0.3910, -0.9893,  1.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
