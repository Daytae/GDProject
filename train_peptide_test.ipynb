{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f54ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Supress pytorch pickle load warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Library imports\n",
    "import gdiffusion as gd\n",
    "import util\n",
    "import util.chem as chem\n",
    "import util.visualization as vis\n",
    "import util.stats as gdstats\n",
    "\n",
    "import datasets as ds\n",
    "\n",
    "import h5py\n",
    "import wandb\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Literal\n",
    "\n",
    "from torch.optim import Adam\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "device = util.util.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "\n",
    "DIFFUSION_PATH = \"saved_models/diffusion/molecule-diffusion-v1.pt\"\n",
    "SELFIES_VAE_PATH = \"saved_models/selfies_vae/selfies-vae.ckpt\"\n",
    "SELFIES_VOCAB_PATH = \"saved_models/selfies_vae/vocab.json\"\n",
    "\n",
    "PEPTIDE_VAE_PATH = \"saved_models/peptide_vae/peptide-vae.ckpt\"\n",
    "PEPTIDE_VOCAB_PATH = \"saved_models/peptide_vae/vocab.json\"\n",
    "\n",
    "LOGP_PREDICTOR_PATH = \"saved_models/logp/model-logp\"\n",
    "PEPTIDE_DATASET_PATH = \"data/peptide_dataset.h5\"\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model : gd.LatentDiffusionModel, # diffusion model to train\n",
    "        dataloader: DataLoader, # latent dataset to train on\n",
    "        eval_fn, # function to run when we evaluate model\n",
    "        eval_every = 10, # we should evaluate the model with eval_fn every X examples\n",
    "\n",
    "        train_num_examples = 100, # number of examples to train on\n",
    "            \n",
    "        save_every = 10, # we should save model every X examples\n",
    "        save_model_folder = \"train/\", # where to save the model\n",
    "\n",
    "        train_lr=1e-4,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        max_grad_norm=1.0,\n",
    "        device = None\n",
    "    ): \n",
    "        self.device = util.util.get_device(device=device)\n",
    "        self.model = diffusion_model\n",
    "        self.dataloader = gd.diffusion.util.cycle(dataloader)\n",
    "\n",
    "        # evaluation\n",
    "        self.eval_fn = eval_fn\n",
    "        self.eval_every = eval_every\n",
    "        \n",
    "        self.train_batch_size = dataloader.batch_size\n",
    "        self.train_num_examples = train_num_examples\n",
    "        \n",
    "        self.save_every = save_every\n",
    "        self.save_model_folder = Path(save_model_folder)\n",
    "        self.save_model_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr, betas=adam_betas)\n",
    "        self.ema = EMA(diffusion_model, beta=ema_decay, update_every=ema_update_every)\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ema.to(self.device)\n",
    "\n",
    "        # progress bars\n",
    "        self.step = 0\n",
    "        self.dataset_len = len(dataloader.dataset) # hopefully is cached lola\n",
    "        self.log_every = -1 # no logging \n",
    "        self.is_logging = self.log_every > 0\n",
    "\n",
    "        print(f\"Loaded {diffusion_model.latent_dim}-dimension latent diffusion model, optimizing {diffusion_model.objective}\")\n",
    "        print(f\"Dataset has {self.dataset_len} elements, training will proceed on {self.train_num_examples} of them\")\n",
    "        print(f\"Trainer will save every {self.save_every} to {self.save_model_folder} and evaluate every {self.eval_every}\")\n",
    "        print(f\"Call .init_wandb() to initialize wandb logging\")\n",
    "        \n",
    "    def init_wandb(self, \n",
    "        log_every: int, \n",
    "        name: str = None, \n",
    "        log_type: Literal['gradients', 'parameters', 'all'] | None = None,\n",
    "        project=\"Guided Diffusion Project v2\",\n",
    "        wandb_dir=\"/train/wandb\"\n",
    "    ):\n",
    "        \n",
    "        ''' Not called in __init__(), must be called manually'''\n",
    "        self.log_every = log_every\n",
    "        self.is_logging = True\n",
    "\n",
    "        wandb.init(\n",
    "            project=project,\n",
    "            dir=wandb_dir,\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "        if log_type:\n",
    "            wandb.watch(self.model, log=log_type)\n",
    "\n",
    "    def _get_model_name(self, milestone):\n",
    "        return str(self.save_model_folder / f'peptide_model_v1-{milestone}.pt')\n",
    "    \n",
    "    def save(self, milestone):\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.model.state_dict(),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'version': \"3.0.0\"\n",
    "        }\n",
    "\n",
    "        torch.save(data, self._get_model_name(milestone))\n",
    "\n",
    "    def load(self, milestone, model_name=None):\n",
    "        model_name = model_name if model_name is not None else self._get_model_name(milestone)\n",
    "        print(f\"Loading to device={self.device}...\")\n",
    "        data = torch.load(self._get_model_name(milestone), map_location=self.device, weights_only=False)\n",
    "\n",
    "        self.model.load_state_dict(data['model'])\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "\n",
    "    def _train_model_step(self):\n",
    "        self.model.train()\n",
    "        self.ema.ema_model.train()\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        data = next(self.dataloader).to(self.device)\n",
    "        \n",
    "        loss = self.model(data)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)\n",
    "\n",
    "        self.opt.step()\n",
    "        return loss_item\n",
    "\n",
    "    def _eval_model(self):\n",
    "        print(\"Attempting evaluation routine:\")\n",
    "        try:\n",
    "            self.ema.ema_model.eval()\n",
    "            self.model.eval()\n",
    "            self.eval_fn(self.model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Encounterred error in sampling routine, aborting\")\n",
    "            print(e)\n",
    "\n",
    "    def _save_model(self):\n",
    "        self.ema.ema_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            milestone = self.step // self.save_every\n",
    "            print(f\"Saving {self._get_model_name(milestone=milestone)} to {self.save_model_folder}\")\n",
    "            self.save(milestone=milestone)\n",
    "\n",
    "    def _log_model(self, loss, pbar):\n",
    "        pbar.set_description(f'loss: {loss:.4f}')\n",
    "        log_dict = {\n",
    "            \"train/loss\" : loss,\n",
    "            \"train/step\": self.step,\n",
    "            \"train/epoch\": self.step // self.dataset_len,\n",
    "            \"train/learning_rate\": self.opt.param_groups[0]['lr'],\n",
    "        }\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    def train(self):\n",
    "        if self.is_logging:\n",
    "            print(\"Warning: wandb logging is not enabled, this may not be intentional\")\n",
    "\n",
    "        with tqdm(initial=self.step, total=self.train_num_examples) as pbar:\n",
    "            while self.step < self.train_num_examples:\n",
    "                if self.step % self.save_every == 0:\n",
    "                    self._save_model()\n",
    "\n",
    "                if self.step % self.eval_every == 0:\n",
    "                    self._eval_model()\n",
    "\n",
    "                # train model for 1 step\n",
    "                loss = self._train_model_step()\n",
    "\n",
    "                if self.step % self.log_every == 0 and self.is_logging:\n",
    "                    self._log_model(loss, pbar)\n",
    "\n",
    "                self.ema.update()\n",
    "                self.step += 1\n",
    "                pbar.update(1)\n",
    "                    \n",
    "        print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09e8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created successfully\n",
      "- Total parameters: 225,056,257\n",
      "- Trainable parameters: 225,056,257\n",
      "- Model size: 858.5 MB\n",
      "- Device: cuda:0\n",
      "- Model Name: LatentDiffusionModel\n",
      "loading model from saved_models/peptide_vae/peptide-vae.ckpt\n",
      "Enc params: 2,675,904\n",
      "Dec params: 360,349\n"
     ]
    }
   ],
   "source": [
    "peptide_dataset = ds.PeptideDataset(file_loc=PEPTIDE_DATASET_PATH)\n",
    "peptide_latent_dataset = ds.LatentDataset(file_loc=PEPTIDE_DATASET_PATH, latent_dim=256)\n",
    "peptide_dataloader = DataLoader(\n",
    "    dataset=peptide_latent_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "diffusion_model = gd.create_diffusion_model(\n",
    "    unet_dim=256, \n",
    "    diffusion_latent_dim=256, \n",
    "    objective='pred_v', \n",
    "    beta_schedule=gd.BetaScheduleSigmoid, \n",
    "    clip_denoised=False, \n",
    "    clip_min=-3.0,  \n",
    "    clip_max=3.0, \n",
    "    model_path=None, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "vae = gd.load_vae_peptides(PEPTIDE_VAE_PATH, PEPTIDE_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f255ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(diffusion_model: gd.LatentDiffusionModel, vae=vae, is_logging=False):\n",
    "    batch_size = 32\n",
    "\n",
    "    z1 = diffusion_model.sample(batch_size=batch_size)\n",
    "    # z2 = diffusion_model.sample(batch_size=batch_size)\n",
    "    rand1 = torch.randn(size=(batch_size, 256))\n",
    "    rand2 = torch.randn(size=(batch_size, 256))\n",
    "\n",
    "    _, random_control_p = gdstats.is_different_from_other(z=rand1, z_other=rand2)\n",
    "    _, diffusion_versus_random_p = gdstats.is_different_from_other(z=z1, z_other=rand1)\n",
    "\n",
    "    # TODO: Convert z1 and compare it to a random peptide and see if there is similarity\n",
    "\n",
    "    # TODO: Diffuse with extinct and see if extinct % goes up\n",
    "\n",
    "    eval_dict = {\n",
    "        \"eval/random_control_p\" : random_control_p,\n",
    "        \"eval/diffusion_versus_random_p\" : diffusion_versus_random_p,\n",
    "    }\n",
    "\n",
    "    wandb.log(eval_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c526d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256-dimension latent diffusion model, optimizing pred_v\n",
      "Dataset has 14774723 elements, training will proceed on 100 of them\n",
      "Trainer will save every 50 to train and evaluate every 10\n",
      "Call .init_wandb() to initialize wandb logging\n"
     ]
    }
   ],
   "source": [
    "trainer = DiffusionTrainer(\n",
    "    diffusion_model = diffusion_model,\n",
    "    dataloader = peptide_dataloader,\n",
    "    eval_fn = evaluate_model,\n",
    "    eval_every = 10,\n",
    "    train_num_examples = 100,\n",
    "    save_every = 50,\n",
    "    save_model_folder= \"train/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from saved_models/peptide_vae/peptide-vae.ckpt\n",
      "Enc params: 2,675,904\n",
      "Dec params: 360,349\n"
     ]
    }
   ],
   "source": [
    "# trainer.load(milestone)\n",
    "# trainer.init_wandb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e99a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256-dimension latent diffusion model, optimizing pred_v\n",
      "Dataset has 14774723 elements, training will proceed on 100 of them\n",
      "Trainer will save every 50 to train and evaluate every 10\n",
      "Call .init_wandb() to initialize wandb logging\n",
      "Attempting evaluation routine:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# trainer.train()\n",
    "trainer._eval_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
