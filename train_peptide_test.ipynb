{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f54ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Supress pytorch pickle load warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Library imports\n",
    "import gdiffusion as gd\n",
    "import util\n",
    "import util.chem as chem\n",
    "import util.visualization as vis\n",
    "import util.stats as gdstats\n",
    "\n",
    "import datasets as ds\n",
    "\n",
    "import h5py\n",
    "import wandb\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Literal\n",
    "\n",
    "from torch.optim import Adam\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "device = util.util.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "\n",
    "DIFFUSION_PATH = \"saved_models/diffusion/molecule-diffusion-v1.pt\"\n",
    "SELFIES_VAE_PATH = \"saved_models/selfies_vae/selfies-vae.ckpt\"\n",
    "SELFIES_VOCAB_PATH = \"saved_models/selfies_vae/vocab.json\"\n",
    "\n",
    "PEPTIDE_VAE_PATH = \"saved_models/peptide_vae/peptide-vae.ckpt\"\n",
    "PEPTIDE_VOCAB_PATH = \"saved_models/peptide_vae/vocab.json\"\n",
    "\n",
    "LOGP_PREDICTOR_PATH = \"saved_models/logp/model-logp\"\n",
    "PEPTIDE_DATASET_PATH = \"data/peptide_dataset.h5\"\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcc0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_dataset = ds.PeptideDataset(file_loc=PEPTIDE_DATASET_PATH)\n",
    "peptide_latent_dataset = ds.LatentDataset(file_loc=PEPTIDE_DATASET_PATH, latent_dim=256)\n",
    "peptide_dataloader = DataLoader(\n",
    "    dataset=peptide_latent_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ec2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created successfully\n",
      "- Total parameters: 225,056,257\n",
      "- Trainable parameters: 225,056,257\n",
      "- Model size: 858.5 MB\n",
      "- Device: cuda:0\n",
      "- Model Name: LatentDiffusionModel\n"
     ]
    }
   ],
   "source": [
    "diffusion_model = gd.create_diffusion_model(\n",
    "    unet_dim=256, \n",
    "    diffusion_latent_dim=256, \n",
    "    objective='pred_v', \n",
    "    beta_schedule=gd.BetaScheduleSigmoid, \n",
    "    clip_denoised=False, clip_min=-3.0,  \n",
    "    clip_max=3.0, \n",
    "    model_path=None, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468d5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model : gd.LatentDiffusionModel, # diffusion model to train\n",
    "        dataloader: DataLoader, # latent dataset to train on\n",
    "        eval_fn, # function to run when we evaluate model\n",
    "        eval_every = 10, # we should evaluate the model with eval_fn every X examples\n",
    "\n",
    "        train_num_examples = 100, # number of examples to train on\n",
    "            \n",
    "        save_every = 10, # we should save model every X examples\n",
    "        save_model_folder = \"train/\", # where to save the model\n",
    "\n",
    "\n",
    "        train_lr=1e-4,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        max_grad_norm=1.0,\n",
    "        device = None\n",
    "    ): \n",
    "        self.device = util.util.get_device(device=device)\n",
    "        self.model = diffusion_model\n",
    "        self.dataloader = gd.diffusion.util.cycle(dataloader)\n",
    "\n",
    "        # evaluation\n",
    "        self.eval_fn = eval_fn\n",
    "        self.eval_every = eval_every\n",
    "        \n",
    "        self.train_batch_size = dataloader.batch_size\n",
    "        self.train_num_examples = train_num_examples\n",
    "        \n",
    "        self.save_every = save_every\n",
    "        self.save_model_folder = Path(save_model_folder)\n",
    "        self.save_model_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr, betas=adam_betas)\n",
    "        self.ema = EMA(diffusion_model, beta=ema_decay, update_every=ema_update_every)\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ema.to(self.device)\n",
    "\n",
    "        # progress bars\n",
    "        self.step = 0\n",
    "        self.dataset_len = len(dataloader.dataset) # hopefully is cached lola\n",
    "        self.log_every = -1 # no logging \n",
    "        self.is_logging = self.log_every > 0\n",
    "\n",
    "        print(f\"Loaded {diffusion_model.latent_dim}-dimension latent diffusion model, optimizing {diffusion_model.objective}\")\n",
    "        print(f\"Dataset has {self.dataset_len} elements, training will proceed on {self.train_num_examples} of them\")\n",
    "        print(f\"Trainer will save every {self.save_every} to {self.save_model_folder} and evaluate every {self.eval_every}\")\n",
    "        print(f\"Call .init_wandb() to initialize wandb logging\")\n",
    "        \n",
    "    def init_wandb(self, \n",
    "        log_every: int, \n",
    "        name: str = None, \n",
    "        log_type: Literal['gradients', 'parameters', 'all'] | None = None,\n",
    "        project=\"Guided Diffusion Project v2\",\n",
    "        wandb_dir=\"/train/wandb\"\n",
    "    ):\n",
    "        \n",
    "        ''' Not called in __init__(), must be called manually'''\n",
    "        self.log_every = log_every\n",
    "        self.is_logging = True\n",
    "\n",
    "        wandb.init(\n",
    "            project=project,\n",
    "            dir=wandb_dir,\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "        if log_type:\n",
    "            wandb.watch(self.model, log=log_type)\n",
    "\n",
    "    def _get_model_name(self, milestone):\n",
    "        return str(self.save_model_folder / f'peptide_model_v1-{milestone}.pt')\n",
    "    \n",
    "    def save(self, milestone):\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.model.state_dict(),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'version': \"3.0.0\"\n",
    "        }\n",
    "\n",
    "        torch.save(data, self._get_model_name(milestone))\n",
    "\n",
    "    def load(self, milestone, model_name=None):\n",
    "        model_name = model_name if model_name is not None else self._get_model_name(milestone)\n",
    "        print(f\"Loading to device={self.device}...\")\n",
    "        data = torch.load(self._get_model_name(milestone), map_location=self.device, weights_only=False)\n",
    "\n",
    "        self.model.load_state_dict(data['model'])\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "\n",
    "    def _train_model_step(self):\n",
    "        self.model.train()\n",
    "        self.ema.ema_model.train()\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        data = next(self.dataloader).to(self.device)\n",
    "        \n",
    "        loss = self.model(data)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)\n",
    "\n",
    "        self.opt.step()\n",
    "        return loss_item\n",
    "\n",
    "    def _eval_model(self):\n",
    "        print(\"Attempting evaluation routine:\")\n",
    "        try:\n",
    "            self.ema.ema_model.eval()\n",
    "            self.model.eval()\n",
    "            self.eval_fn(self.model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Encounterred error in sampling routine, aborting\")\n",
    "            print(e)\n",
    "\n",
    "    def _save_model(self):\n",
    "        self.ema.ema_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            milestone = self.step // self.save_every\n",
    "            print(f\"Saving {self._get_model_name(milestone=milestone)} to {self.save_model_folder}\")\n",
    "            self.save(milestone=milestone)\n",
    "\n",
    "    def _log_model(self, loss, pbar):\n",
    "        pbar.set_description(f'loss: {loss:.4f}')\n",
    "        log_dict = {\n",
    "            \"train/loss\" : loss,\n",
    "            \"train/step\": self.step,\n",
    "            \"train/epoch\": self.step // self.dataset_len,\n",
    "            \"train/learning_rate\": self.opt.param_groups[0]['lr'],\n",
    "        }\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    def train(self):\n",
    "        if self.is_logging:\n",
    "            print(\"Warning: wandb logging is not enabled, this may not be intentional\")\n",
    "\n",
    "        with tqdm(initial=self.step, total=self.train_num_examples) as pbar:\n",
    "            while self.step < self.train_num_examples:\n",
    "                if self.step % self.save_every == 0:\n",
    "                    self._save_model()\n",
    "\n",
    "                if self.step % self.eval_every == 0:\n",
    "                    self._eval_model()\n",
    "\n",
    "                # train model for 1 step\n",
    "                loss = self._train_model_step()\n",
    "\n",
    "                if self.step % self.log_every == 0 and self.is_logging:\n",
    "                    self._log_model(loss, pbar)\n",
    "\n",
    "                self.ema.update()\n",
    "                self.step += 1\n",
    "                pbar.update(1)\n",
    "                    \n",
    "        print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba54f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from saved_models/peptide_vae/peptide-vae.ckpt\n",
      "Enc params: 2,675,904\n",
      "Dec params: 360,349\n"
     ]
    }
   ],
   "source": [
    "vae = gd.load_vae_peptides(PEPTIDE_VAE_PATH, PEPTIDE_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e99a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256-dimension latent diffusion model, optimizing pred_v\n",
      "Dataset has 14774723 elements, training will proceed on 100 of them\n",
      "Trainer will save every 50 to train and evaluate every 10\n",
      "Call .init_wandb() to initialize wandb logging\n",
      "Attempting evaluation routine:\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model: gd.LatentDiffusionModel, vae=vae):\n",
    "    \n",
    "\n",
    "trainer = DiffusionTrainer(\n",
    "    diffusion_model = diffusion_model,\n",
    "    dataloader = peptide_dataloader,\n",
    "    eval_fn = evaluate_model,\n",
    "    eval_every = 10,\n",
    "    train_num_examples = 100,\n",
    "    save_every = 50,\n",
    "    save_model_folder= \"train/\"\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "trainer._eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from ema_pytorch import EMA\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import selfies as sf\n",
    "from fcd import get_fcd\n",
    "\n",
    "import gdutil\n",
    "import gdiffusion\n",
    "\n",
    "class Trainer1D():\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model: gdiffusion.GaussianDiffusion1D,\n",
    "        dataset: Dataset,\n",
    "        vae=None,\n",
    "        *,\n",
    "        train_batch_size = 16,\n",
    "        train_num_steps = 100,\n",
    "        sample_batch_size = 32,\n",
    "\n",
    "        save_and_sample_every = 10,\n",
    "        log_every = 1,\n",
    "        results_folder = \"train/saved_models\",\n",
    "\n",
    "        project_name=\"GuidedDiffusionProject-1\",\n",
    "        run_name=\"Diffusion Train Attempt #433\",\n",
    "\n",
    "        train_lr=1e-4,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "\n",
    "        device=None\n",
    "    ):\n",
    "        if not device:\n",
    "            self._device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self._device = device\n",
    "        \n",
    "        # model\n",
    "        self.model = diffusion_model\n",
    "        dl = DataLoader(dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "        self.dl = gdiffusion.util.cycle(dl)\n",
    "\n",
    "        self.vae = vae\n",
    "\n",
    "        # batch size + train length\n",
    "        self.batch_size = train_batch_size\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.sample_batch_size= sample_batch_size\n",
    "\n",
    "        # logging\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "        self.log_every = log_every\n",
    "\n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            name=run_name\n",
    "        )\n",
    "\n",
    "        wandb.watch(diffusion_model, log=\"all\", log_freq=100)\n",
    "\n",
    "\n",
    "        # saving\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr, betas=adam_betas)\n",
    "        self.ema = EMA(diffusion_model, beta=ema_decay, update_every=ema_update_every)\n",
    "        self.ema.to(self.device)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        self.dataset_len = len(dataset)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "    \n",
    "    def _get_model_name(self, milestone):\n",
    "        return str(self.results_folder / f'modelv2-{milestone}.pt')\n",
    "    \n",
    "    def _get_sample_name(self, milestone):\n",
    "        return str(self.results_folder / f'samplev2-{milestone}.npy')\n",
    "    def save(self, milestone):\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.model.state_dict(),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'version': \"2.0.0\"\n",
    "        }\n",
    "\n",
    "        torch.save(data, self._get_model_name(milestone))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        print(f\"Loading to device={self.device}...\")\n",
    "        data = torch.load(self._get_model_name(milestone), map_location=self.device, weights_only=True)\n",
    "\n",
    "        self.model.load_state_dict(data['model'])\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "\n",
    "    def train(self):\n",
    "        with tqdm(initial=self.step, total=self.train_num_steps) as pbar:\n",
    "            while self.step < self.train_num_steps:\n",
    "                self.model.train()\n",
    "                total_loss = 0.\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                data = next(self.dl).to(self.device)\n",
    "\n",
    "                loss = self.model(data)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                self.opt.step()\n",
    "\n",
    "                self.step += 1\n",
    "\n",
    "                # Logging\n",
    "                if self.step % self.log_every == 0:\n",
    "                    self.log(total_loss)\n",
    "               \n",
    "                # EMA\n",
    "                self.ema.update()\n",
    "\n",
    "                if self.step % self.save_and_sample_every == 0 or self.step == 1:\n",
    "                    self.ema.ema_model.eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        milestone = self.step // self.save_and_sample_every\n",
    "                        print(f\"Saving milestone: {milestone} model\")\n",
    "                        self.save(milestone=milestone)\n",
    "\n",
    "                    try:\n",
    "                        print(\"Attempting sample routine...\")\n",
    "                        self.sample(milestone=milestone)\n",
    "                    except Exception as e:\n",
    "                        print(\"Sampling routine FAILED!\")\n",
    "                        print(e)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(\"Training Complete!\")\n",
    "        wandb.finish()\n",
    "\n",
    "    def log(self, total_loss):\n",
    "        log_dict = {\n",
    "                \"train/loss\" : total_loss,\n",
    "                \"train/step\": self.step,\n",
    "                \"train/epoch\": self.step / self.dataset_len,\n",
    "                \"train/learning_rate\": self.opt.param_groups[0]['lr'],\n",
    "            }\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    def _sample_diffusion_smiles(self, num_batches=2, batch_size=32):\n",
    "\n",
    "        # sample num_batch batches and collate them\n",
    "        smiles_list_list = []\n",
    "        for _ in range(num_batches):\n",
    "            # diffusion output --> prior\n",
    "\n",
    "            z = gdiffusion.sample_diffusion(model=self.model, batch_size=batch_size) # repl with sample\n",
    "            selfies_ = gdutil.latent_to_selfies(z, vae=self.vae)\n",
    "            smiles = [sf.decoder(s) for s in selfies_]\n",
    "            smiles_list_list.append(smiles)\n",
    "        diffusion_smiles = [smile for smile_list in smiles_list_list for smile in smile_list]\n",
    "        return diffusion_smiles\n",
    "\n",
    "    def _sample_vae_smiles(self, num_batches=2, batch_size=32):\n",
    "\n",
    "        # sample num_batch batches and collate them\n",
    "        smiles_list_list = []\n",
    "        for _ in range(num_batches):\n",
    "\n",
    "            # vae prior --> smiles\n",
    "            z = torch.randn(size=(batch_size, 128))\n",
    "            selfies_ = gdutil.latent_to_selfies(z, vae=self.vae)\n",
    "            smiles = [sf.decoder(s) for s in selfies_]\n",
    "            smiles_list_list.append(smiles)\n",
    "        vae_smiles = [smile for smile_list in smiles_list_list for smile in smile_list]\n",
    "        return vae_smiles\n",
    "    \n",
    "    def _sample_wrong_dist(self, num_batches=2, batch_size=32):\n",
    "        # sample num_batch batches and collate them\n",
    "        # from a uniform prior\n",
    "\n",
    "        smiles_list_list = []\n",
    "        for _ in range(num_batches):\n",
    "\n",
    "            # vae prior --> smiles\n",
    "            z = torch.rand(size=(batch_size, 128))\n",
    "            selfies_ = gdutil.latent_to_selfies(z, vae=self.vae)\n",
    "            smiles = [sf.decoder(s) for s in selfies_]\n",
    "            smiles_list_list.append(smiles)\n",
    "        vae_smiles = [smile for smile_list in smiles_list_list for smile in smile_list]\n",
    "        return vae_smiles\n",
    "    \n",
    "    \n",
    "    def sample(self, milestone):\n",
    "        # latents = gdiffusion.sample_diffusion(model=self.model, batch_size=self.sample_batch_size)\n",
    "        \n",
    "        # try:\n",
    "        #     np.save(self._get_sample_name(milestone=milestone), latents.cpu().numpy())\n",
    "        #     print(f\"milestone {milestone} results saved successfully\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "\n",
    "        if self.vae is not None:\n",
    "            try:\n",
    "                diffusion_smiles = self._sample_diffusion_smiles()\n",
    "                vae_smiles = self._sample_vae_smiles()\n",
    "                vae_smiles_control = self._sample_vae_smiles()\n",
    "\n",
    "                off_distribution = self._sample_wrong_dist()\n",
    "\n",
    "                experiment_fcd = get_fcd(diffusion_smiles, vae_smiles)\n",
    "                print(f\" Experimental FCD: {experiment_fcd}\")\n",
    "\n",
    "                control_fcd = get_fcd(vae_smiles, vae_smiles_control)\n",
    "                print(f\" Control FCD: {control_fcd}\")\n",
    "\n",
    "                off_distribution_fcd = get_fcd(vae_smiles, off_distribution)\n",
    "                print(f\" Off Distribution FCD: {off_distribution_fcd}\")\n",
    "\n",
    "                data = {\n",
    "                    \"val/fcd-diffusion-to-vae\": experiment_fcd,\n",
    "                    \"val/fcd-vae-to-vae\": control_fcd,\n",
    "                    \"val/fcd-experiment-minus-control\": experiment_fcd - control_fcd,\n",
    "                    \"val/fcd-vae_prior-to-uniform-control\": off_distribution_fcd,\n",
    "                }\n",
    "\n",
    "                wandb.log(data)                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"VAE could not be located, skipping FCD validation\")\n",
    "\n",
    "    def load_sample(self, milestone):\n",
    "        sample = np.load(self._get_sample_name(milestone=milestone))\n",
    "        return sample\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
